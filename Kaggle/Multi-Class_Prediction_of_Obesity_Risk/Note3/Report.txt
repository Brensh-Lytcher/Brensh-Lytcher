2024/05/01~03
Plan
  1. モデルの可視化方法を調べる
  2. 決定木(Decision Trees)モデルの学習と実践
  3. ランダムフォレスト(Random Forest)モデルの学習と実践
  4. ニューラルネットワーク(Neural Networks)モデルの学習と実践

Do
  1. モデルの可視化
      -決定境界の可視化
        func: mlxtend.plotting.plot_decision_regions()
        parameters:
          -X:array-like, shape=[n_samples, n_features]
            説明変数(numpy.ndarray)
          -y:array-like, shape=[n_samples]
            目的変数(numpy.ndarray)
          -clf:Classifer object
            分類モデル
          -feature_index:array-like(default:(0,) for 1D, (0,1) otherwise)
            x,y軸の特徴量
          -filler_feature_values:dict(default:None)
            特徴量(説明変数)が3つ以上の場合, 未使用の特徴量の値を指定
          -filler_feature_ranges
            filler_feature_valuesで指定した値からのずれの範囲を指定
          -ax:matplotlib.axes.Axes(default:None)
            matplotlibのaxと同様
          -X_highlight:array-like, shape=[n_samples, n_features](default:None)
            ハイライト表示するデータを指定
          -zoom_factor:float(default:1.0)
            不明
          -hide_spines:bool(default:True)
            軸の表示
          -legend:int(default:1)
            凡例の位置
          -markers:str(default:'s^oxv<>')
            データの記号
          -colors:str(default:'red,blue,limegreen,gray,cyan')
            色の指定
          -scatter_kwargs:dict(default:None)
            不明。matplotlibのscatterと関係があるもの
          -contourf_kwargs:dict(default:None)
            不明。matplotlibのcontourfと関係があるもの
          -contour_kwargs:dict(default:None)
            不明。matplotlibのcontourと関係があるもの
          -scatter_highlight_kwargs:dict(default:None)
            不明。matplotlibのscatterと関係があるもの
          -n_jobs:int or None, optional(default=None)
            計算に使用するCPUの数


        func: seaborn_analyzer.classplot.class_separator_plot()
        parameters:
          -clf(classifier object implementing)
            モデル
          -x:List[str], or np.ndarray
            説明変数
          -y:str or np.ndarray
            目的変数
          -data:pd.DataFrame
            入力データ
          -x_colnames:List[str], default=None
            dataがpd.DataFrameでない場合に入力データのラベルリストを指定する
          -x_chart:List[str], default=None
            説明変数の内のグラフ表示対象
          -pair_sigmarange:float, default=1.0
            グラフ表示しない説明変数の分割範囲
          -pair_sigmainterval:float, default=0.5
            グラフ表示しない説明変数の1枚当たりの表示範囲
          -chart_extendsigma:float, default=0.5
            グラフx,y軸の表示拡張範囲
          -char_scale:int, default=1
            グラフ解像度倍率(大きいほど描画速度が速くなるが、画像が粗くなる)
          -plot_scatter=:{'error', 'class', 'class_error', None}, default='class_error'
            散布図の描画種類。errorは分類正誤、classはクラスを色表示、class_errorは各クラスの分類正誤の色表示
          -rounddigit_x3:int, default=2
            グラフ表示しない変数の小数丸め桁数
          -scatter_colors:List[str], default=None
            各クラスのデータの色(plot_scatterがclassまたはclass_errorのときのみに使える)
          -true_marker:str, default='o'
            正しく分類されたデータのマーク
          -false_marker:str, default='x'
            誤分類されたデータのマーク
          -cv:int, cross-validation generator, or an iterable, default=None
            クロスバリデーション分割法を決める。Noneの場合は5-fold cross validation, intの場合はKFoldを使用。
          -cv_seed:int, default=42
            cross validationの乱数シード
          -cv_group:str, default=None
            グループ分割における対象カラム名
          -display_cv_indices:int, default=0
            クロスバリデーション分割の番号
          -clf_params:dict, default=None
            モデルに渡すパラメータ
          -fit_params:dict, default=None
            モデル学習時のパラメータ
          -eval_set_selection:{'all', 'test', 'train', 'original', 'original_transformed'}, default=None
            モデル学習に使うデータを指定
          -subplot_kws:dict, default=None
            matplotlib.pyplot.subplotsに渡す引数
          -contourf_kws:dict, default=None
            matplotlib.pyplot.contourf(描画用)に渡す引数
          -scatter_kws:dict, default=None
            matplotlib.pyplot.scatterに渡す引数
          -legend_kws:dict, default=None
            凡例指定
            
        mlxtend.plotting.plot_decision_regions()はndarray型のデータしか扱えないので、前処理が必要。それに対して、seaborn_analyzer.classplot.class_proba_plot()は前処理の必要がないので、使いやすい

      -クラス確率の可視化
        func: seaborn_analyzer.classplot.class_proba_plot()
        parameters:
          seaborn_analyzer.classplot.class_separator_plot()のparameterをすべて持つ。そのほかのparameterは以下の通りである。
          -plot_border:bool, default=True
            分類の境界線の表示
          -proba_class:str, List[str], default=None
            確率表示となる対象のクラス名(指定クラスの確率のみを表示する)
          -proba_cmap_dict:dict[str,str], default=None
            各クラスの確率図の色(データの色ではなく,範囲の背景色)指定
          -proba_type:{'contourf', 'contour', 'imshow'}, default='contourf'
            確率図の種類の指定。imshowはRGB, contourfは塗りつぶしあり等高線, contourは塗りつぶしなし等高線
          -imshow_kws:dict, default=None
            matplotlib.pyplot.imshowに渡す引数(proba_type='imshow'のときのみに使用可能)

  2. 決定木(Decision Trees)
      木構造を使って分類する。決定木の最上層のノードを根ノード, 最下層のノードを葉ノードという。また、あるノードから見て, その上のノードが親ノード, その下のノードが子ノードである。
      model: sklearn.tree.DecisionTreeClassifier()
      parameters:
        -criterion:{“gini”, “entropy”, “log_loss”}, default=”gini”
          データ分割の評価基準
        -splitter:{“best”, “random”}, default=”best”
          各ノードでの分割方法
        -max_depth:int, default=None
          最大深度
        -min_samples_split:int or float, default=2
          ノードを分割するための最小限サンプルサイズ。整数の場合はその数自体、少数の場合は全サンプルサイズに対する割合で指定する
        -min_samples_leaf:int or float, default=1
          葉を構成するための最小限サンプル数。整数の場合はその数自体、少数の場合は全サンプルサイズに対する割合で指定する
        -min_weight_fraction_leaf:float, default=0.0
          葉ノードに必要な重みの最小加重割合(各サンプルの重みを自分で指定。指定しなければ、全サンプルの重みを均等とみなす。)
        -max_features:int, float or {“sqrt”, “log2”}, default=None
          考慮する特徴量の数の指定。整数の場合はその数、少数の場合は全体の割合、autoは2乗根、log2はlog2(全特徴量数)。
        -random_state:int, RandomState instance or None, default=None
          乱数シード
        -max_leaf_nodes:int, default=None
          葉の数の最大限指定。(小さくすることで過学習防止)
        -min_impurity_decrease:float, default=0.0
          学習の早期停止閾値。(大きくすることで過学習防止)
        -class_weight:dict, list of dict or “balanced”, default=None
          各クラスの重みの設定。min_weight_fraction_leafで使う。
        -ccp_alpha:non-negative float, default=0.0
          木の複雑さを小さくするもの
        -monotonic_cst:array-like of int of shape (n_features), default=None
          各特徴量の単調性制約指定。1は単調増加, 0は制約なし, -1は単調減少
      決定木分類法が硬くて、確率で決めるべきデータに柔軟に対応しにくく、過学習しやすいと考えられる。一方、カテゴリデータを対応しやすいと考えられる。よって、連続データや確率に対応しやすいモデルと組み合わせて使った方がよい。
      max_leaf_nodesやclass_weightなどのパラメータをどう指定すべきかが問題である。パラメータの異なる組み合わせ方をすべて試したら、最適解を見つけ出すのにかなり時間がかかる。一方、自分で調べるとしたら、データの間の相関計算やモデルの可視化、データの分布調査などがとても複雑である。
      決定木を使って、特徴量の重要度を評価することができる。その仕組みは、決定木の中で、それぞれの特徴量がどれくらい多くのノードに使われ、どれくらい大きな効果があるかによって重要度を計算する。特徴量重要度の値はtree.feature_importances_にある。ただし、treeは学習した決定木モデルである。

  3. ランダムフォレスト(Random Forest)
      ランダムフォレストアンサンブルの手法を使って、複数の決定木モデルを統合して、各モデルの予測結果に基づいて、多数決で最終結果を決める。それぞれの決定木モデルに入れる特徴量がランダムに選ばれ、学習データもランダムに抽出されるため、お互いの欠点を補正することができ、過学習になりにくい。ただし、特徴量抽出は非復元抽出、データセット作成は復元抽出。
      ただし、ランダムフォレストの使用にあたって、特徴量の多重共線性を排除すべきだ。つまり、互いに相関が極めて大きいデータを削除すべきだ。このような特徴量を使ったら、各特徴量の重要度の評価が大きく影響されて、取り入れるべき特徴量を見逃す可能性がある。
      model: sklearn.ensemble.RandomForestClassifier()
      parameters:
        -criterion:{“gini”, “entropy”, “log_loss”}, default=”gini”
        -max_depth:int, default=None
        -min_samples_split:int or float, default=2
        -min_samples_leaf:int or float, default=1
        -min_weight_fraction_leaf:float, default=0.0
        -max_features:{“sqrt”, “log2”, None}, int or float, default=”sqrt”
        -max_leaf_nodes:int, default=None
        -min_impurity_decrease:float, default=0.0
        -random_state:int, RandomState instance or None, default=None
        -class_weight:{“balanced”, “balanced_subsample”}, dict or list of dicts, default=None
        -ccp_alpha:non-negative float, default=0.0
        -monotonic_cstarray-like of int of shape (n_features), default=None
          以上はDecisionTreeClassifierと同じ

        -n_estimators:int, default=100
          決定木モデルの数
        -bootstrap:bool, default=True
          ブートストラップデータセットを作成するか。(各モデルに専用のデータセットを作成するか。)
        -oob_score:bool or callable, default=False
          ブートストラップ法でデータセットを作成するときに、1回も使われないデータをoobデータと呼ぶ。oobデータを使ってモデルの精度を評価するか。
        -n_jobs:int, default=None
          モデルを学習させるときに使うジョブの数
        -verbose:int, default=0
          モデル学習の詳細を表示する
        -warm_start:bool, default=False
          前に作成した決定木を再利用するか
        -max_samples:int or float, default=None
          ブートストラップを使うとき、各決定木に使うデータセットのサンプル数。Noneの場合、入力データのサンプル数と同じサンプル数のデータセットを作成する。

      ランダムフォレストは決定木モデルを組み合わせたものなので、その精度が決定木より高くなるはず。なので、決定木の練習をしなくてもよい。また、ランダムフォレストも特徴量の重要度を評価できる。
      
            
Check
  1. モデルの決定境界とクラス確率の可視化について、グラフは2次元のデータしか表示できないので、グラフをたくさん表示しなければ決定境界を理解できない。また、決定境界の様子から、モデルが過学習になっているかが推測できるので、過学習抑止に役立つ。しかし、決定境界とクラス確率の可視化の関数の実行に大量の時間がかかるため、有効に活用することができない。
  2. 決定木が過学習になりやすくて、パラメータ指定も複雑であるため、予測にあまり役に立たないだろうと思った。しかし、モデル統合には使えるかもしれない。
  3. ランダムフォレストは数百個の決定木を組み合わせたモデルであるため、計算時間が長かった。これは最適なパラメータを見つけることを妨げる。また、複数モデルを統合する手法はアンサンブルの手法と呼ばれることがわかった。今後、モデル統合をするために、アンサンブルの手法を調べて学習すべきだ。

Action


参考文献
[1] Qiita: Pythonでデータの挙動を見やすくする可視化ツールを作成してみた（分類編）, https://qiita.com/c60evaporator/items/43866a42e09daebb5cc0, 2024/05/02
[2] mlxtend: plot_decision_regions: Visualize the decision regions of a classifier, https://rasbt.github.io/mlxtend/user_guide/plotting/plot_decision_regions/, 2024/05/02
[3] 清水琢也、小川雄太郎: AIエンジニアを目指す人のための機械学習入門，技術評論社，p66-81
[4] scikit-learn: sklearn.tree.DecisionTreeClassifier, https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html, 2024/05/02
[5] scikit-learn: sklearn.ensemble.RandomForestClassifier, https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html, 2024/05/02
[6] 

