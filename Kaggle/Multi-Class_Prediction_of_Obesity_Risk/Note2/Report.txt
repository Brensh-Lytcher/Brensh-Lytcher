2024/04/22~26
Plan
  1. 線形回帰 (Linear Regression)の学習と実践
  2. ロジスティック回帰 (Logistic Regression)の学習と実践
  3. サポートベクターマシン (Support Vector Machines, SVM)の学習と実践
  4. 決定木 (Decision Trees)の学習と実践

Do
  1. 最小二乗法による線形回帰
      -1つの入力変数のみを使う:単回帰モデル
      -複数個の入力変数を使う:重回帰モデル
      回帰直線(平面or超平面): y = w0 + w1*x1 + w2*x2 + ... + wn*xn    (x:特徴量)
        (w0:バイアスパラメータ, w1,w2,...,wn:重みパラメータ    最小二乗法によって定まる)
      評価方法: R^2スコア
                 Σ_i=1 (yi - yi')^2
      R^2 = 1 - --------------------        (yi'がxiに対するyの予測値, μがyの平均値)
                 Σ_i=1 (yi - μ)^2
      model: sklearn.linear_model.LinearRegression()  [2]
      Parameters:
        -fit_intercept:bool, default=True
          バイアスパラメータw0の使用
        -copy_X:bool, default=True
          入力データをコピーして使用(Falseにすると、モデルを作成した後に、入力データも変わってしまう)
        -n_jobs:int, default=None
          計算に使用するジョブ数(Noneは1. -1にすると、すべてのCPUを使って計算)
        -positive:bool, default=False
          係数(パラメータw)を正数にする
          
      線形回帰モデルの過学習抑止: L1正則化(Lasso), L2正則化(Ridge回帰)
      L1正則化(Lasso)
        大量の特徴量の内の不要なものを除く(重みパラメータwiを0にする)
        誤差関数: E(w) = 1/2*Σ_i=1 (yi - yi')^2  + α*Σ_i=0 |wi|
        E(w)を最小にするような係数w0,w1,...,wnを求める
        model: sklearn.linear_model.Lasso()  [3]
        parameters: 
          -alpha:float, default=1.0
            E(w)のαを指定
          -fit_intercept:bool, default=True
            LinearRegressionと同じ
          -precompute:bool or array-like of shape (n_features, n_features), default=False
            グラム行列の使用(計算を高速化させる)
          -copy_X:bool, default=True
            LinearRegressionと同じ
          -max_iter:int, default=1000
            誤差を最小化する計算の繰り返し回数
          -tol:float, default=1e-4
            誤算の許容範囲(どれくらい最適化させるか)
          -warm_start:bool, default=False
            前回の解を初期値とする
          -positive:bool, default=False
            LinearRegressionと同じ
          -random_state:int, RandomState instance, default=None
            乱数シード
          -selection:{‘cyclic’, ‘random’}, default=’cyclic’
            係数の更新順序(cyclic:順番  random:ランダム)

      L2正則化(Ridge回帰)
        大きな特徴量にペナルティを与えて、重みを調整す、
        誤差関数: E(w) = 1/2*Σ_i=1 (yi - yi')^2  + α*Σ_i=0 wi^2
        E(w)を最小にするような係数w0,w1,...,wnを求める
        model: sklearn.linear_model.Ridge()  [4]
        parameters:
          -alpha:{float, ndarray of shape (n_targets,)}, default=1.0
          -fit_intercept:bool, default=True
          -copy_X:bool, default=True
          -max_iter:int, default=None
          -tol:float, default=1e-4
          -solver:{‘auto’, ‘svd’, ‘cholesky’, ‘lsqr’, ‘sparse_cg’, ‘sag’, ‘saga’, ‘lbfgs’}, default=’auto’
            使用する計算アルゴリズム
          -positive:bool, default=False
          -random_state:int, RandomState instance, default=None

      実践によると、線形回帰は連続な値の予測に有利なモデルであるが、離散な値の予測（予測値を四捨五入などの方法で整数にする）に不利である。今回のコンペ「Multi-Class Prediction of Obesity Risk」の目的変数はObject型であり、離散型の整数で表せるので、線形回帰の予測精度が低いが、説明変数の中に連続型のデータがあるので、説明変数の間の関係の分析に使えるかもしれない。また、他のコンペについて、説明変数に欠損値があることもあるで、その欠損値を補完することに役立つだろう。
      線形回帰は直線型の回帰式が得られるが、n次関数や指数関数、対数関数、三角関数などの曲線に近いデータ分布の場合もあるので、それらに対応できるようなモデルも学ぶべきだ。
      線形回帰について、回帰式を使った分類方法のアイデアを考えた。それは、データの回帰式からのずれの大きさによる分類方法である。ある説明変数x以外の説明変数を使って、xの回帰式を求め、xのi番目のデータx_iと回帰式による推測値x_i'との間の差をd_iとする。このd_iを新しい説明変数として目的変数を予測する。つまり、線形回帰で新しい説明変数を作り、その説明変数を目的変数の予測に使う。

  2. ロジスティック回帰によるクラス分類  [5]
      最適なロジスティクス関数(シグモイド関数)を求めて、説明変数を入力して、出力が0.5以下であれば0に、0.5より大きければ1にする。
      model: sklearn.linear_model.LogisticRegression()  [6]
      parameters:
        -penalty:{‘l1’, ‘l2’, ‘elasticnet’, None}, default=’l2’
          過学習抑止方法, l1がLasso, l2がRidge, elasticnetはl1とl2両方を使う
        -dual:bool, default=False
          双対問題
        -tol:float, default=1e-4
          収束計算の精度
        -C:float, default=1.0
          正則化の強さの逆数
        -fit_intercept:bool, default=True
          定数パラメータの使用
        -intercept_scaling:float, default=1
          合成特徴量
        -class_weight:dict or ‘balanced’, default=None
          特徴量の重さの設定
        -random_state:int, RandomState instance, default=None
          Lassoと同じ
        -solver:{‘lbfgs’, ‘liblinear’, ‘newton-cg’, ‘newton-cholesky’, ‘sag’, ‘saga’}, default=’lbfgs’
          計算方法。小さなデータセットにはliblnear、大きなデータセットにはsag, saga、複数クラスの問題にはnewton-cg, sag, saga, lbfgsが使われる
        -max_iter:int, default=100
          Lassoと同じ
        -multi_class:{‘auto’, ‘ovr’, ‘multinomial’}, default=’auto’
          多クラス分類の方法。
        -verbose:int, default=0
          冗長性の設定
        -warm_start:bool, default=False
          Lassoと同じ
        -n_jobs:int, default=None
          LinearRegressionと同じ
        -l1_ratio:float, default=None
          penaltyにelasticnetが使われるときのl1の比率

      実践において、max_iterが小さすぎれば、warningが出ることに気づいた。
      n_jobsが大きくなるほど、計算時間が短くなるとは限らないことを発見した。
      max_iterを増加させると、予測の精度が上がるが、15000を超えたら、ほとんど上昇しなくなった。
      ロジスティック回帰はロジスティクス関数による出力の大きさによって分類する方法であるため、出力が0~0.2と0.8~1のデータの予測精度が高いが、出力が0.2~0.8のデータの予測精度が低いことが明らかである。この特徴を利用した新しい分類方法のアイデアを考えた。1つ目は、データを予測精度によって、高精度グループと低精度グループの2つに分けて、高精度グループの予測値をそのまま確定し、低精度グループを別のモデルあるいは別のロジスティクス関数で再び予測する。2つ目は、ロジスティクス関数の出力を使って新しい精度変数を作る。この精度変数を予測値の重みとして、別のモデルによる予測値と結合する。どちらの方法でも、予測精度の高いものを守りながら、予測精度の低いものの精度を上げることができる、と考えられる。

  3. サポートベクターマシンによるクラス分類
      サポートベクターマシン(SVM)には線形SVMと非線形SVMがある。
      -線形SVC
        マージンとペナルティによって分類の境界線となる直線（平面・超平面）を決める。マージンとは境界線とそれぞれのクラスの最も近いデータとの間の距離である。また、それらのデータをサポートベクトルという。ペナルティとは誤分類されたデータとその実際に所属するクラスのサポートベクトルの間の距離である。マージンを最大にしながら、ペナルティ総和を最小にするような直線（平面）を分類の境界とする。
        誤差関数: E = 1/2 * (Σ_k=1 wk^2) + C * (Σ_i=1 ζi)
        分類条件: yi * (w1 * x1_i + w2 * x2_i + ... + wk * xk_i + b) >= 1 - ζi
        (注) wkがk番目の特徴量の係数, xk_iがk番目の特徴量のi番目のデータ, ζiがi番目データのペナルティ(誤分類されていなければζi=0), Cがペナルティ総和の係数(マージンとペナルティの重みの調整), yiがi番目データのクラス(-1 or 1)
        model: sklearn.svm.LinearSVC()  [7]
        parameters:
          -penalty:{‘l1’, ‘l2’}, default=’l2’
            ペナルティの基準
          -loss:{‘hinge’, ‘squared_hinge’}, default=’squared_hinge’
            損失関数
          -dual: “auto” or bool, default=True
            アルゴリズムの選択
          -tol:float, default=1e-4
            停止基準の許容範囲
          -C:float, default=1.0
            正式化パラメーター(誤算関数中のC)
          -multi_class:{‘ovr’, ‘crammer_singer’}, default=’ovr’
            3つ以上のクラスがある場合の戦略決定。crammer_singerは精度向上が少なく、計算コストが高いので、ほとんど使わない
          -fit_intercept:bool, default=True
            切片の使用(分類条件の中のbの使用)
          -intercept_scaling:float, default=1.0
            切片スケーリング
          -class_weight:dict or ‘balanced’, default=None
            クラスiのパラメータCをclass_weight[i] * Cに設定する
          -verbose:int, default=0
            途中プロセスの表示
          -random_state:int, RandomState instance or None, default=None
            乱数シード
          -max_iter:int, default=1000
            計算の繰り返しの最大数
            
      -非線形SVC
        非線形SVCは線形SVCの方法にカーネル法(データを高次元特徴空間に写像する)を加えた。
        modle: sklearn.svm.SVC()
        parameters:
          -C:float, default=1.0
            LinearSVCと同じ
          -kernel={‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’} or callable, default=’rbf’
            カーネルのタイプ
          -degree:int, default=3
            kernel=polyのときのみ使用できる。polynomialの次数を指定する。
          -gamma:{‘scale’, ‘auto’} or float, default=’scale’
            kernel=rbf, poly, sigmoidのとき使用できる。カーネル関数中の定数γを指定する。
          -coef0:float, default=0.0
            kernel=poly, sigmoidのとき重要。カーネル関数の独立項
          -shrinking:bool, default=True
            heuristicを下げる(計算速度調節)
          -probability:bool, default=False
            予測確率の計算
          -tol:float, default=1e-3
            LinearSVCと同じ
          -cache_size:float, default=200
            カーネルのキャッシュサイズ(計算速度調節)
          -class_weight:dict or ‘balanced’, default=None
            LinearSVCと同じ
          -verbose:bool, default=False
            LinearSVCと同じ
          -max_iter:int, default=-1
            LinearSVCと同じ
          -decision_function_shape:{‘ovo’, ‘ovr’}, default=’ovr’
            多クラス分類の方法
          -break_ties:bool, default=False
            ?意味不明
          -random_state:int, RandomState instance or None, default=None
            乱数シード

      パラメーターCについて、Cが大きいほどLinearSVCのスコアが低くなったが、SVCのスコアが高くなった。ただし、場合によって、Cがある値を超えるとLinearSVCのスコアが再び上がることがある(random_stateによって異なる)。
      パラメーターmax_iterについて、max_iterが大きいほど、SVCのスコアが高くなったが、LinearSVCのスコアがほとんど変わらなかった。また、max_iterが3000以上のとき、SVCのスコアがほとんど変わらなくなった。そして、max_iterが低いときに、LinearSVCのスコアがSVCより高いが、max_iterが大きくなると、LinearSVCのスコアがSVCより低くなった。つまり、SVCに必要な計算量がLinearSVCより高いが、極限精度もLinearSVCより高い。
      SVCのパラメーターkernelについて、polyとrbfのスコアがとても高かったが、linearとsigmodのスコアが極めて低かった。また、precomputedは正方行列の型のデータにしか使えないことも分かった。これはおそらく画像処理に使うものだろう。
      SVCのパラメーターcoef0の値を0から10まで上げると、最初はスコアが上がり、coef0=2前後で極大値をとり、それ以降、スコアが下がることを発見した。
      SVCのパラメーターtolについて、tol=0.1~1のときのスコアが最も高かったが、他の数値を代入した場合との差がかなり小さかった(0.003くらい)ので、今回のコンペにおいてtolが重要なものではないと考えられるが、他のコンペではどうなるかもわかれば、tolに対する理解も深まるだろう。
      異なるクラスのデータ分布が一部重なっている場合、重なった部分の予測精度が低くなるはずだ。このことについて、2つのアイデアを考えた。1つ目は、データと境界線の距離がある基準値を超えた場合に高精度グループ、超えない場合に低精度グループに分ける方法である。しかし、一部の領域において、高精度グループと低精度グループを区別する基準値が高いけど、他の領域での基準値が低い場合が考えられるので、場所によらず基準値をすべて同じ値に決めることは不適切だ。2つ目は、SVCの関数を調整して、それぞれのクラスのデータのみが分布する領域の境界線を見つける。つまり、2つのクラスAとBに対して、A\BとBの境界線およびB\AとAの境界線を見つける。この2本の境界線がデータを(A\B)と(AかつB)と(B\A)の3つの領域に分ける。しかし、SVCの関数が複雑すぎて、まだ理解できないので、SVC関数の調整方法がわからない。

Check
  1. Planで4つのモデルの学習と実践を計画したが、4つ目の決定木モデルをやっていなかった。計画をすべて完成しなかった理由として、3つ挙げられる。1つ目は、モデルの仕組み・関数とパラメーターが複雑であり、それを理解するのに大量の時間をかけたためだ。2つ目は、1つのモデルを学習したときに、それに関連する別のモデルも一緒に学習したので、実際の学習量が予想より多かったためだ。3つ目は、モデルの実践において、モデルの実行時間が長かったためだ。特に、max_iterが大きくなるほど、計算時間も長くなる。
  2. 今度のコンペにおいて、線形回帰は目的変数の予測に使えない（不利）が、特徴量の作成における応用が考えられる。ロジスティック回帰は予測精度がわかるため、低精度のデータを取り出して再予測すること、つまりモデルの組み合わせにおける応用が考えらえる。サポートベクターマシンは簡単に使えるが、その中身が複雑すぎるので、応用が難しい。
  3. 各モデルによる分類ができたが、その分類の境界線や回帰線がどのようになっていたかがわからないので、モデルの改善と応用が難しい。よって、各モデルが作った回帰線と境界線の可視化方法も学ばなければならない。

Action
  1. 現在の進度がスケジュールより遅れているので、ゴールデンウイークに作業をどんどん進めるべきだ。
  2. モデルの応用・組み合わせ方法をたくさん考えたので、この後の「モデルを組み合わせる方法の学習」における作業量がかなり増えるので、スケジュールを調整すべきだ。
  3. スケジュールによると、この後はモデルの学習を続けるつもりであったが、モデルを理解するために、モデルが作った境界線・回帰線の可視化スキルが不可欠なので、モデルの学習を続ける前に、モデルの可視化スキルを先に習得すべきだ。

参考文献
[1] 清水琢也、小川雄太郎: AIエンジニアを目指す人のための機械学習入門，技術評論社，p23-65
[2] scikit-learn: sklearn.linear_model.LinearRegression, https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html
[3] scikit-learn: sklearn.linear_model.Lasso, https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html
[4] scikit-learn: sklearn.linear_model.Ridge, https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html
[5] DATA SCIENCE LIFE: ロジスティック回帰を多クラス分類に応用する【機械学習入門17】, https://datawokagaku.com/multinomial/
[6] scikit-learn: sklearn.linear_model.LogisticRegression, https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html
[7] scikit-learn: sklearn.svm.LinearSVC, https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html
[8] scikit-learn: sklearn.svm.SVC, https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html
