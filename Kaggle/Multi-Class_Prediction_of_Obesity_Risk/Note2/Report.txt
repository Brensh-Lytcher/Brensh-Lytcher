2024/04/22~26
Plan
  1. 線形回帰 (Linear Regression)の学習と実践
  2. ロジスティック回帰 (Logistic Regression)の学習と実践
  3. サポートベクターマシン (Support Vector Machines, SVM)の学習と実践
  4. 決定木 (Decision Trees)の学習と実践

Do
  1. 最小二乗法による線形回帰
      -1つの入力変数のみを使う:単回帰モデル
      -複数個の入力変数を使う:重回帰モデル
      回帰直線(平面or超平面): y = w0 + w1*x1 + w2*x2 + ... + wn*xn    (x:特徴量)
        (w0:バイアスパラメータ, w1,w2,...,wn:重みパラメータ    最小二乗法によって定まる)
      評価方法: R^2スコア
                 Σ_i=1 (yi - yi')^2
      R^2 = 1 - --------------------        (yi'がxiに対するyの予測値, μがyの平均値)
                 Σ_i=1 (yi - μ)^2
      model: sklearn.linear_model.LinearRegression()  [2]
      Parameters:
        -fit_intercept:bool, default=True
          バイアスパラメータw0の使用
        -copy_X:bool, default=True
          入力データをコピーして使用(Falseにすると、モデルを作成した後に、入力データも変わってしまう)
        -n_jobs:int, default=None
          計算に使用するジョブ数(Noneは1. -1にすると、すべてのCPUを使って計算)
        -positive:bool, default=False
          係数(パラメータw)を正数にする
          
      線形回帰モデルの過学習抑止: L1正則化(Lasso), L2正則化(Ridge回帰)
      L1正則化(Lasso)
        大量の特徴量の内の不要なものを除く(重みパラメータwiを0にする)
        誤差関数: E(w) = 1/2*Σ_i=1 (yi - yi')^2  + α*Σ_i=0 |wi|
        E(w)を最小にするような係数w0,w1,...,wnを求める
        model: sklearn.linear_model.Lasso()  [3]
        parameters: 
          -alpha:float, default=1.0
            E(w)のαを指定
          -fit_intercept:bool, default=True
            LinearRegressionと同じ
          -precompute:bool or array-like of shape (n_features, n_features), default=False
            グラム行列の使用(計算を高速化させる)
          -copy_X:bool, default=True
            LinearRegressionと同じ
          -max_iter:int, default=1000
            誤差を最小化する計算の繰り返し回数
          -tol:float, default=1e-4
            誤算の許容範囲(どれくらい最適化させるか)
          -warm_start:bool, default=False
            前回の解を初期値とする
          -positive:bool, default=False
            LinearRegressionと同じ
          -random_state:int, RandomState instance, default=None
            乱数シード
          -selection:{‘cyclic’, ‘random’}, default=’cyclic’
            係数の更新順序(cyclic:順番  random:ランダム)

      L2正則化(Ridge回帰)
        大きな特徴量にペナルティを与えて、重みを調整す、
        誤差関数: E(w) = 1/2*Σ_i=1 (yi - yi')^2  + α*Σ_i=0 wi^2
        E(w)を最小にするような係数w0,w1,...,wnを求める
        model: sklearn.linear_model.Ridge()  [4]
        parameters:
          -alpha:{float, ndarray of shape (n_targets,)}, default=1.0
          -fit_intercept:bool, default=True
          -copy_X:bool, default=True
          -max_iter:int, default=None
          -tol:float, default=1e-4
          -solver:{‘auto’, ‘svd’, ‘cholesky’, ‘lsqr’, ‘sparse_cg’, ‘sag’, ‘saga’, ‘lbfgs’}, default=’auto’
            使用する計算アルゴリズム
          -positive:bool, default=False
          -random_state:int, RandomState instance, default=None

      実践によると、線形回帰は連続な値の予測に有利なモデルであるが、離散な値の予測（予測値を四捨五入などの方法で整数にする）に不利である。今回のコンペ「Multi-Class Prediction of Obesity Risk」の目的変数はObject型であり、離散型の整数で表せるので、線形回帰の予測精度が低いが、説明変数の中に連続型のデータがあるので、説明変数の間の関係の分析に使えるかもしれない。また、他のコンペについて、説明変数に欠損値があることもあるで、その欠損値を補完することに役立つだろう。
      線形回帰は直線型の回帰式が得られるが、n次関数や指数関数、対数関数、三角関数などの曲線に近いデータ分布の場合もあるので、それらに対応できるようなモデルも学ぶべきだ。

  2. ロジスティック回帰によるクラス分類  [5]
      最適なロジスティクス関数(シグモイド関数)を求めて、説明変数を入力して、出力が0.5以下であれば0に、0.5より大きければ1にする。
      model: sklearn.linear_model.LogisticRegression()  [6]
      parameters:
        -penalty:{‘l1’, ‘l2’, ‘elasticnet’, None}, default=’l2’
          過学習抑止方法, l1がLasso, l2がRidge, elasticnetはl1とl2両方を使う
        -dual:bool, default=False
          双対問題
        -tol:float, default=1e-4
          収束計算の精度
        -C:float, default=1.0
          正則化の強さの逆数
        -fit_intercept:bool, default=True
          定数パラメータの使用
        -intercept_scaling:float, default=1
          合成特徴量
        -class_weight:dict or ‘balanced’, default=None
          特徴量の重さの設定
        -random_state:int, RandomState instance, default=None
          Lassoと同じ
        -solver:{‘lbfgs’, ‘liblinear’, ‘newton-cg’, ‘newton-cholesky’, ‘sag’, ‘saga’}, default=’lbfgs’
          計算方法。小さなデータセットにはliblnear、大きなデータセットにはsag, saga、複数クラスの問題にはnewton-cg, sag, saga, lbfgsが使われる
        -max_iter:int, default=100
          Lassoと同じ
        -multi_class:{‘auto’, ‘ovr’, ‘multinomial’}, default=’auto’
          多クラス分類の方法。
        -verbose:int, default=0
          冗長性の設定
        -warm_start:bool, default=False
          Lassoと同じ
        -n_jobs:int, default=None
          LinearRegressionと同じ
        -l1_ratio:float, default=None
          penaltyにelasticnetが使われるときのl1の比率

      実践において、max_iterが小さすぎれば、warningが出ることに気づいた。
      n_jobsが大きくなるほど、計算時間が短くなるとは限らないことを発見した。
      max_iterを増加させると、予測の精度が上がるが、10000くらいを超えたら、ほとんど上昇しなくなった。

  3. サポートベクターマシンによるクラス分類
      サポートベクターマシン(SVM)には線形SVMと非線形SVMがある。
      -線形SVC
        マージンとペナルティによって分類の境界線となる直線（平面・超平面）を決める。マージンとは境界線とそれぞれのクラスの最も近いデータとの間の距離である。また、それらのデータをサポートベクトルという。ペナルティとは誤分類されたデータとその実際に所属するクラスのサポートベクトルの間の距離である。マージンを最大にしながら、ペナルティ総和を最小にするような直線（平面）を分類の境界とする。
        誤差関数: E = 1/2 * (Σ_k=1 wk^2) + C * (Σ_i=1 ζi)
        分類条件: yi * (w1 * x1_i + w2 * x2_i + ... + wk * xk_i + b) >= 1 - ζi
        (注) wkがk番目の特徴量の係数, xk_iがk番目の特徴量のi番目のデータ, ζiがi番目データのペナルティ(誤分類されていなければζi=0), Cがペナルティ総和の係数(マージンとペナルティの重みの調整), yiがi番目データのクラス(-1 or 1)
        model: sklearn.svm.LinearSVC()  [7]
        parameters:
          -penalty:{‘l1’, ‘l2’}, default=’l2’
            ペナルティの基準
          -loss:{‘hinge’, ‘squared_hinge’}, default=’squared_hinge’
            損失関数
          -dual: “auto” or bool, default=True
            アルゴリズムの選択
          -tol:float, default=1e-4
            停止基準の許容範囲
          -C:float, default=1.0
            正式化パラメーター(誤算関数中のC)
          -multi_class:{‘ovr’, ‘crammer_singer’}, default=’ovr’
            3つ以上のクラスがある場合の戦略決定。crammer_singerは精度向上が少なく、計算コストが高いので、ほとんど使わない
          -fit_intercept:bool, default=True
            切片の使用(分類条件の中のbの使用)
          -intercept_scaling:float, default=1.0
            切片スケーリング
          -class_weight:dict or ‘balanced’, default=None
            クラスiのパラメータCをclass_weight[i] * Cに設定する
          -verbose:int, default=0
            途中プロセスの表示
          -random_state:int, RandomState instance or None, default=None
            乱数シード
          -max_iter:int, default=1000
            計算の繰り返しの最大数
            
      -非線形SVC
        非線形SVCは線形SVCの方法にカーネル法(データを高次元特徴空間に写像する)を加えた。
        modle: sklearn.svm.SVC()
        parameters:
          -C:float, default=1.0
            LinearSVCと同じ
          -kernel={‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’} or callable, default=’rbf’
            カーネルのタイプ
          -degree:int, default=3
            kernel=polyのときのみ使用できる。polynomialの次数を指定する。
          -gamma:{‘scale’, ‘auto’} or float, default=’scale’
            kernel=rbf, poly, sigmoidのとき使用できる。カーネル関数中の定数γを指定する。
          -coef0:float, default=0.0
            kernel=poly, sigmoidのとき重要。カーネル関数の独立項
          -shrinking:bool, default=True
            heuristicを下げる(計算速度調節)
          -probability:bool, default=False
            予測確率の計算
          -tol:float, default=1e-3
            LinearSVCと同じ
          -cache_size:float, default=200
            カーネルのキャッシュサイズ(計算速度調節)
          -class_weight:dict or ‘balanced’, default=None
            LinearSVCと同じ
          -verbose:bool, default=False
            LinearSVCと同じ
          -max_iter:int, default=-1
            LinearSVCと同じ
          -decision_function_shape:{‘ovo’, ‘ovr’}, default=’ovr’
            多クラス分類の方法
          -break_ties:bool, default=False
            ?意味不明
          -random_state:int, RandomState instance or None, default=None
            乱数シード

      パラメーターCについて、Cが大きいほどLinearSVCのスコアが低くなったが、SVCのスコアが高くなった。
      パラメーターmax_iterについて、max_iterが大きいほど、SVCのスコアが高くなったが、LinearSVCのスコアがほとんど変わらなかった。また、max_iterが3000以上のとき、SVCのスコアがほとんど変わらなくなった。そして、max_iterが低いときに、LinearSVCのスコアがSVCより高いが、max_iterが大きくなると、LinearSVCのスコアがSVCより低くなった。つまり、SVCに必要な計算量がLinearSVCより高いが、極限精度もLinearSVCより高い。
      SVCのパラメーターkernelについて、polyとrbfのスコアがとても高かったが、linearとsigmodのスコアが極めて低かった。また、precomputedは正方行列の型のデータにしか使えないことも分かった。これはおそらく画像処理に使うものだろう。

Check
  1. また、今度のコンペに線形回帰は役に立たないので、今後は連続値を予測するコンペに参加して、それを実践すべきだ。
  2. 
  4. 

Action



参考文献
[1] 清水琢也、小川雄太郎: AIエンジニアを目指す人のための機械学習入門，技術評論社，p23-65
[2] scikit-learn: sklearn.linear_model.LinearRegression, https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html
[3] scikit-learn: sklearn.linear_model.Lasso, https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html
[4] scikit-learn: sklearn.linear_model.Ridge, https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html
[5] DATA SCIENCE LIFE: ロジスティック回帰を多クラス分類に応用する【機械学習入門17】, https://datawokagaku.com/multinomial/
[6] scikit-learn: sklearn.linear_model.LogisticRegression, https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html
[7] scikit-learn: sklearn.svm.LinearSVC, https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html
[8] scikit-learn: sklearn.svm.SVC, https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html
