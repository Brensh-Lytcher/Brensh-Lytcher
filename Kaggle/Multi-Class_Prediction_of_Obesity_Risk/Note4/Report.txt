2024/04/30~03
Plan
  1. ニューラルネットワーク(Neural Networks)モデルの学習と実践
  2. K-近傍法(k-Nearest Neighbors, k-NN)モデルの学習と実践
  3. アンサンブル学習の手法の学習（アンサンブルを応用したモデルづくりはスケジュールとおり）
  (まとめた他人のコードの学習・理解は延期)

Do
  1. ニューラルネットワーク(Neural Networks)
      ニューラルネットワークは神経を真似して作られたものである。多層パーセプトロン分類器(MLPClassifier)はニューラルネットワークを使ったモデルであり、入力層と隠れ層と出力層の3種類の層からできている。それぞれの層には複数個のニューロンが平行に並べられていて、1つの層のニューロンが前の層のいくつかのニューロンから入力を受け取って計算し、次の層のいくつかのニューロンに出力を与える。モデルに大量のデータを与えて学習させることで、ニューロン中の各定数の大きさを調整し、スコアができるだけ高い状態に変化させる。
      model: sklearn.neural_network.MLPClassifier()
      parameters:
        -hidden_layer_sizes:array-like of shape(n_layers - 2,), default=(100,)
          隠れ層のサイズ(層の数、ニューロンの数)
          (n1, n2, n3, ..., nk)niは隠れ層のi層目のニューロン数
        -activation:{‘identity’, ‘logistic’, ‘tanh’, ‘relu’}, default=’relu’
          隠れ層の活性化関数
          identity: f(x) = x
          logistic: f(x) = 1/(1 + exp(-x))
          tanh:     f(x) = tanh(x) = (1 - exp(-2x))/(1 + exp(-2x))
          relu:     f(x) = max(0, x)
        -solver:{‘lbfgs’, ‘sgd’, ‘adam’}, default=’adam’
          最適化手法
          lbfgs: 準ニュートン法。1000以下の小さいデータセットの場合に高速トレーニング可能
          lgd  : 確率的勾配降下法。訓練データをランダムにシャフルして重みを更新する
          adam : 期待値計算に指数移動平均を使う
        -alpha:float, default=0.0001
          L2正則化のpenalty
        -batch_size:int, default=’auto’
          solver = 'sgd' or 'adam'のときのミニバッチサイズ
          auto  : batch_size = min(200, n_samples)
        -learning_rate:{‘constant’, ‘invscaling’, ‘adaptive’}, default=’constant’
          重みの学習率の更新仕方(solver = 'sgd'のときのみ)
          constant   : 学習率を変更しない
          invscaling : 時間に伴って学習率を小さくする
          adaptive   : 最初は学習率を固定し、lossの減少量が連続2回tolより小さければ、学習率を1/5に減らす
        -learning_rate_init:float, default=0.001
          重みの学習率の初期値(solver = 'sgd' or 'adam')
        -power_t:float, default=0.5
          学習率の減少速度(learning_rate = invscalingのときのみ)
        -max_iter:int, default=200
          学習の反復最大回数
        -shuffle:bool, default=True
          反復学習のときのデータシャフル
        -random_state:int, RandomState instance, default=None
          乱数シード
        -tol:float, default=1e-4
          学習の収束値。大きすぎると学習途中で終了、小さすぎると過学習
        -verbose:bool, default=False
          学習進歩状況の表示
        -warm_start:bool, default=False
          前回が学習結果の利用
        -momentum:float, default=0.9
          solver = 'sgd'のときのみ
          重みの修正量に前回の重みの修正量を加算する
          0~1の値をとる
        -nesterovs_momentum:bool, default=True
          momentum > 0のときのみ
          学習率更新においてmomentumを考慮するか
        -early_stopping:bool, default=False
          学習終了の使用(solver = 'sgd' or 'adam')
        -validation_fraction:float, default=0.1
          検証用データの割合
        -beta_1:float, default=0.9
          solver = 'adam'のときのみ
          adamの式のβ1の値
        -beta_2:float, default=0.999
          solver = 'adam'のときのみ
          adamの式のβ2の値
        -epsilon:float, default=1e-8
          solver = 'adam'のときのみ
          adamの式のεの値
        -n_iter_no_change:int, default=10
          tolで収束しない場合の最大学習回数(solver = 'sgd' or 'adam')
        -max_fun:int, default=15000
          誤差関数の最大値(solver = 'lbfgs')

      考察:
      -実践により、MLPClassifierの計算時間がほかのモデルより長かったことに気づいた。特に、隠れ層のサイズを(3, 300)以上にすると、一回の計算で数分間もかかるので、最適なパラメータの組み合わせを見つけるのが非常に困難、つまりモデルの力を完全に発揮することが難しい。そして、モデルの内部のそれぞれのニューロンが予測にどんな役割を果たしているかがわからないので、予測結果の分析ができない。
      -隠れ層の数hとニューロンの数nが大きいほど、スコアが高くなる。hが大きいほど、nを大きくするときのスコアの増加率が大きくなる一方、スコアの変化が不安定になる(スコアの分布が広くなる)。ランダムフォレストのように、アンサンブル学習の手法で複数のニューラルネットワークモデルを統合すると、スコアを安定化させることが可能だろうと思ったが、計算時間が長すぎるため、アンサンブル学習による応用が難しいだろう。
      -learning_rate_initが小さいほどスコアが高くなると予想したが、実際はそうなっていなかった。また、学習率の更新方法は計算時間を減らすことに有効であると予想したが、学習率更新方法'constant'と'invscaling'による予測結果が同じであったため、予想とおりにはいかなかった。learning_rate_initの数をより多く、その範囲を広くして、隠れ層のサイズを大きくして再検証しようと思ったが、計算時間が長すぎたため、それをやめた。

  2. K-近傍法(k-Nearest Neighbors, k-NN)
      予測データの近くのK個の学習データの最も一般的なクラスに分類する。距離の算出には一般にユーグリッド距離が使われる(ほかにマンハッタン距離もある)。高次元データに不利である(予測速度が遅くなる)。
      model: sklearn.neighbors.KNeighborsClassifier()
      parameters:
        -n_neighbors:int, default=5
          kの値(近くのデータ数)
          大きくすると精度が下がり、小さくするとノイズに弱い
        -weights:{‘uniform’, ‘distance’}, callable or None, default=’uniform’
          重みの設定
          uniform  : 均一な重み
          distance : 距離に応じた重み
        -algorithm:{‘auto’, ‘ball_tree’, ‘kd_tree’, ‘brute’}, default=’auto’
          最も近いデータを選ぶアルゴリズム
          ball_tree : ball treeアルゴリズム
          kd_tree   : kd treeアルゴリズム
          brute     : 総当たり探索
          auto      : 自動選択(最適アルゴリズムに)
        -leaf_size:int, default=30
          algorithm = 'ball_tree' or 'kd_tree'のときのリーフサイズ
        -p:float, default=2
          ミンコフスキー距離
          p = 1 : マンハッタン距離
          p = 2 : ユーグリッド距離
        -metric:str or callable, default=’minkowski’
          距離の測定方法
        -metric_params:dict, default=None
          metricの追加
        -n_jobs:int, default=None
          計算に使うjob数(CPUの数)

      考察:
      -実践により、n_neighborsが10前後のとき、スコアが最も高かった。また、データの重みを距離によって設定(weights = 'distance')する場合のスコアが均一重みの場合より高かった。また、マンハッタン距離を使った場合のスコアがユーグリッド距離を使った場合より高くなっていた。
      -K近傍法の計算速度がとても速かったので、最適なパラメータの組み合わせを見つけることが容易にできる。また、計算速度速いというメリットがあるため、アンサンブル学習によってより複雑なモデルを作ることに有利だろう。
      -k近傍法の出力が予測値になっているが、近くのk個のデータの距離とk個のデータそれぞれの所属クラスに基づいて、予測値の重み(予測確率)が求められる。この重みはk近傍法の応用に使えるだろうと思った。

  3. アンサンブル学習
      アンサンブル学習とは、複数の学習モデルを組み合わせて1つの学習モデルを作成する方法である。アンサンブル学習に重要なのはバイアスとバリアンスである。バイアスとは予測値と実際の値の誤差を表すものであり、バリアンスとは予測値の散らばり具合である。バイアスが大きいほど、予測精度が高い。また、バリアンスが大きいほど、予測値の散らばりが小さいが、過学習の可能性が大きくなる。
      アンサンブル学習の代表的な手法として、バギング、ブースティングとスタッキングが挙げられる。
      -バギング
        バギングは、異なるデータを使って複数の異なるモデルを作成し、それらの平均を最終モデルとする方法である。その特徴はバリアンスが小さいことと学習時間が短いことである。バギングは決定木と一緒に使われることが多い(ランダムフォレスト)。
      -ブースティング
        ブースティングは、同じデータを使って、前のモデルで誤分類されたデータに重みをつけて次のモデルに学習させることを繰り返す方法である。その特徴はバイアスを小さいが、学習時間が長くて、過学習の確率が大きいことである。XGBoostとLightGBMはブースティングを使った代表的なモデルである。
      -スタッキング
        スタッキングは複数の異なるのモデルを作って、それらの予測値を特徴量として別のモデルで再予測する方法である。その特徴は予測精度が高いが、分類の仕組み・結果が解釈・分析しにくくて、学習時間が長いことである。

      考察:
      -それぞれのモデルにはメリットとデメリットがあり、同じモデルであっても学習データの順番とモデル内部のランダム値(random_stateなど)によってモデルが変わるので、それぞれのモデルには予測精度が高いデータ範囲と予測精度が低いデータ範囲がある。バギングは異なるモデルの出力をそのまま重ねて平均値をとるため、あるモデルの予測精度の低い部分をほかのモデルで埋めることができるが、その予測精度の高い部分も他のモデルによって精度が引き下げられてしまう可能性があると思った。これを改善するために、データの予測精度に応じて、異なる場所に分布するデータに重みをつける方法が考えられる。ブースティングはあるモデルの予測精度の低い部分に注目して、他のモデルで再学習することでその部分の予測精度を改善する。しかし、同じデータセットを使い続けるため、そのデータセットに過度に依存し、それ以外のデータに対応できなくなる、つまり過学習になりやすい。データ数が極めて大きくて、ノイズも少ないデータセットに対して、ブースティングが有利になるが、そうでないデータセットに対して不利になるはずだ。スタッキングはそれぞれのモデルの出力を特徴量として別のモデルで再予測するため、バギングのように出力結果の多数決による誤差が小さくなるが、それぞれのモデルの予測精度の高い部分と低い部分が分析できないため、その出力は特徴量としての価値が小さくなる。仮に、それぞれのモデルの予測値ではなく、データのがそれぞれのクラスに分類される確率を特徴量としたら(つまり、それぞれの予測値に重みをかける)、この問題は解決できるだろう。

Check
  1. MLPClassifierの隠れ層のサイズ、学習率初期値、学習方法などとスコアの関係を調べた。しかし、計算時間が長すぎたため、他のパラメータの効果を調べていなかった。また、ニューラルネットワークには主にディープニューラルネットワーク(DNN)、再帰型ニューラルネットワーク(RNN)、畳み込みニューラルネットワーク(CNN)、自己符号化器と敵対的生成ネットワーク(GAN)などがあり、MLPClassifierはDNNである。しかし、自己符号化器とGANは教師なしモデルであり、今回の学習対象外である。CNNは主に画像処理に使われ、RNNは主に自然言語処理に使われるため、とても複雑で、その学習に大量の時間がかかるだろうと予想したので、スケジュールを考慮した上で、その学習をあきらめた。
  2. KNeighborsClassifierの計算速度が速いため、ほとんどのパラメータを実践で使った。
  3. アンサンブル学習について、3つの手法を学習した。また、それぞれの手法の改善点も考えた。

Action
  1. 今度の計画をすべて完成したが、進度がスケジュールより遅れているので、今後はもっと急がなければならない。
  2. ニューラルネットワークモデルの計算が大変時間かかることに気づいた。今後はアンサンブル学習の手法を使うとき、たくさんのモデルを操作しなければならないので、もっと時間がかかるかもしれない。なので、もし可能であれば、スーパーコンピュータの使い方も習いたい。
  3. アンサンブル学習の手法について、その改善方法を考えたが、それらと大きく異なる新しい手法を自分で見つけたい。

参考文献
  -scikit-learn: sklearn.neural_network.MLPClassifier, https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html, 2024/05/09
  -SPJ: ニューラルネットワークのパラメータ設定方法(scikit-learnのMLPClassifier), https://spjai.com/neural-network-parameter/, 2024/05/09
  -scikit-learn: sklearn.neighbors.KNeighborsClassifier, https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html, 2024/05/09
  -Qiita: 機械学習 〜 K−近傍法 〜, https://qiita.com/fujin/items/128ed7188f7e7df74f2c, 2024/05/09
  -Qiita: アンサンブル学習まとめ!!(実装あり), https://qiita.com/hara_tatsu/items/336f9fff08b9743dc1d2, 2024/05/09
