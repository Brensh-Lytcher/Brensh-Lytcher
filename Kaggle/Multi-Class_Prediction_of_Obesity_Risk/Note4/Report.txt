2024/04/30~03
Plan
  1. ニューラルネットワーク(Neural Networks)モデルの学習と実践
  2. K-近傍法(k-Nearest Neighbors, k-NN)モデルの学習と実践
  3. アンサンブル学習の学習（アンサンブルを応用したモデルづくりはスケジュールとおり）
  (まとめた他人のコードの学習・理解は延期)

Do
  1. ニューラルネットワーク(Neural Networks)
      ニューラルネットワークは神経を真似して作られたものである。多層パーセプトロン分類器(MLPClassifier)はニューラルネットワークを使ったモデルであり、入力層と隠れ層と出力層の3種類の層からできている。それぞれの層には複数個のニューロンが平行に並べられていて、1つの層のニューロンが前の層のいくつかのニューロンから入力を受け取って計算し、次の層のいくつかのニューロンに出力を与える。モデルに大量のデータを与えて学習させることで、ニューロン中の各定数の大きさを調整し、スコアができるだけ高い状態に変化させる。
      model: sklearn.neural_network.MLPClassifier()
      parameters:
        -hidden_layer_sizes:array-like of shape(n_layers - 2,), default=(100,)
          隠れ層のサイズ(層の数、ニューロンの数)
          (n1, n2, n3, ..., nk)niは隠れ層のi層目のニューロン数
        -activation:{‘identity’, ‘logistic’, ‘tanh’, ‘relu’}, default=’relu’
          隠れ層の活性化関数
          identity: f(x) = x
          logistic: f(x) = 1/(1 + exp(-x))
          tanh:     f(x) = tanh(x) = (1 - exp(-2x))/(1 + exp(-2x))
          relu:     f(x) = max(0, x)
        -solver:{‘lbfgs’, ‘sgd’, ‘adam’}, default=’adam’
          最適化手法
          lbfgs: 準ニュートン法。1000以下の小さいデータセットの場合に高速トレーニング可能
          lgd  : 確率的勾配降下法。訓練データをランダムにシャフルして重みを更新する
          adam : 期待値計算に指数移動平均を使う
        -alpha:float, default=0.0001
          L2正則化のpenalty
        -batch_size:int, default=’auto’
          solver = 'sgd' or 'adam'のときのミニバッチサイズ
          auto  : batch_size = min(200, n_samples)
        -learning_rate:{‘constant’, ‘invscaling’, ‘adaptive’}, default=’constant’
          重みの学習率の更新仕方(solver = 'sgd'のときのみ)
          constant   : 学習率を変更しない
          invscaling : 時間に伴って学習率を小さくする
          adaptive   : 最初は学習率を固定し、lossの減少量が連続2回tolより小さければ、学習率を1/5に減らす
        -learning_rate_init:float, default=0.001
          重みの学習率の初期値(solver = 'sgd' or 'adam')
        -power_t:float, default=0.5
          学習率の減少速度(learning_rate = invscalingのときのみ)
        -max_iter:int, default=200
          学習の反復最大回数
        -shuffle:bool, default=True
          反復学習のときのデータシャフル
        -random_state:int, RandomState instance, default=None
          乱数シード
        -tol:float, default=1e-4
          学習の収束値。大きすぎると学習途中で終了、小さすぎると過学習
        -verbose:bool, default=False
          学習進歩状況の表示
        -warm_start:bool, default=False
          前回が学習結果の利用
        -momentum:float, default=0.9
          solver = 'sgd'のときのみ
          重みの修正量に前回の重みの修正量を加算する
          0~1の値をとる
        -nesterovs_momentum:bool, default=True
          momentum > 0のときのみ
          学習率更新においてmomentumを考慮するか
        -early_stopping:bool, default=False
          学習終了の使用(solver = 'sgd' or 'adam')
        -validation_fraction:float, default=0.1
          検証用データの割合
        -beta_1:float, default=0.9
          solver = 'adam'のときのみ
          adamの式のβ1の値
        -beta_2:float, default=0.999
          solver = 'adam'のときのみ
          adamの式のβ2の値
        -epsilon:float, default=1e-8
          solver = 'adam'のときのみ
          adamの式のεの値
        -n_iter_no_change:int, default=10
          tolで収束しない場合の最大学習回数(solver = 'sgd' or 'adam')
        -max_fun:int, default=15000
          誤差関数の最大値(solver = 'lbfgs')

      MLPClassifierの計算時間がほかのモデルより長かった。特に、隠れ層のサイズを(3, 200)以上いすると、一回の計算で数分かかるので、最適なパラメータの組み合わせを見つけるのが非常に困難である。
      隠れ層の数hとニューロンの数nが大きいほど、スコアが高くなる。hが大きいほど、nを大きくするときのスコアの増加率が大きくなる一方、スコアの変化が不安定になる(スコアの分布が広くなる)。
      learning_rate_initが小さいほどスコアが高くなると予想したが、実際はそうなっていなかった。また、学習率の更新方法は計算時間を減らすことに有効であると予想したが、学習率更新方法'constant'と'invscaling'による予測結果が同じであったため、予想とおりにはいかなかった。learning_rate_initの数をより多く、その範囲を広くして、隠れ層のサイズを大きくして再検証しようと思ったが、計算時間が長すぎたため、それをやめた。

  2. K-近傍法(k-Nearest Neighbors, k-NN)
      予測データの近くのK個の学習データの最も一般的なクラスに分類する。距離の算出には一般にユーグリッド距離が使われる(ほかにマンハッタン距離もある)。高次元データに不利である(予測速度が遅くなる)。
      model: sklearn.neighbors.KNeighborsClassifier()
      parameters:
        -n_neighbors:int, default=5
          kの値(近くのデータ数)
          大きくすると精度が下がり、小さくするとノイズに弱い
        -weights:{‘uniform’, ‘distance’}, callable or None, default=’uniform’
          重みの設定
          uniform  : 均一な重み
          distance : 距離に応じた重み
        -algorithm:{‘auto’, ‘ball_tree’, ‘kd_tree’, ‘brute’}, default=’auto’
          最も近いデータを選ぶアルゴリズム
          ball_tree : ball treeアルゴリズム
          kd_tree   : kd treeアルゴリズム
          brute     : 総当たり探索
          auto      : 自動選択(最適アルゴリズムに)
        -leaf_size:int, default=30
          algorithm = 'ball_tree' or 'kd_tree'のときのリーフサイズ
        -p:float, default=2
          ミンコフスキー距離
          p = 1 : マンハッタン距離
          p = 2 : ユーグリッド距離
        -metric:str or callable, default=’minkowski’
          距離の測定方法
        -metric_params:dict, default=None
          metricの追加
        -n_jobs:int, default=None
          計算に使うjob数(CPUの数)

      n_neighborsが10前後のとき、スコアが最も高かった。また、データの重みを距離によって設定(weights = 'distance')する場合のスコアが均一重みの場合より高かった。また、マンハッタン距離を使った場合のスコアがユーグリッド距離を使った場合より高くなっていた。
      K近傍法の計算速度がとても速かったので、最適なパラメータの組み合わせを見つけることが容易にできる。

  3. アンサンブル学習
      アンサンブル学習とは、複数の学習モデルを組み合わせて1つの学習モデルを作成する方法である。アンサンブル学習に重要なのはバイアスとバリアンスである。バイアスとは予測値と実際の値の誤差を表すものであり、バリアンスとは予測値の散らばり具合である。バイアスが大きいほど、予測精度が高い。また、バリアンスが大きいほど、予測値の散らばりが小さいが、過学習の可能性が大きくなる。
      アンサンブル学習の代表的な手法として、バギング、ブースティングとスタッキングが挙げられる。
      -バギング
        バギングは、異なるデータを使って複数の異なるモデルを作成し、それらの平均を最終モデルとする方法である。その特徴はバリアンスが小さいことと学習時間が短いことである。バギングは決定木と一緒に使われることが多い(ランダムフォレスト)。
      -ブースティング
        ブースティングは、同じデータを使って、前のモデルで誤分類されたデータに重みをつけて次のモデルに学習させることを繰り返す方法である。その特徴はバイアスを小さいが、学習時間が長くて、過学習の確率が大きいことである。XGBoostとLightGBMはブースティングを使った代表的なモデルである。
      -スタッキング
        スタッキングは複数の異なるのモデルを作って、それらの予測値を特徴量として別のモデルで再予測する方法である。その特徴は予測精度が高いが、分類の仕組み・結果が解釈・分析しにくくて、学習時間が長いことである。

Check
  1. MLPClassifierの隠れ層のサイズ、学習率初期値、学習方法などとスコアの関係を調べた。しかし、計算時間が長すぎたため、他のパラメータの効果を調べていなかった。

Action
  2. 

参考文献
  -scikit-learn: sklearn.neural_network.MLPClassifier, https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html, 2024/05/09
  -SPJ: ニューラルネットワークのパラメータ設定方法(scikit-learnのMLPClassifier), https://spjai.com/neural-network-parameter/, 2024/05/09
  -scikit-learn: sklearn.neighbors.KNeighborsClassifier, https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html, 2024/05/09
  -Qiita: 機械学習 〜 K−近傍法 〜, https://qiita.com/fujin/items/128ed7188f7e7df74f2c, 2024/05/09
  -Qiita: アンサンブル学習まとめ!!(実装あり), https://qiita.com/hara_tatsu/items/336f9fff08b9743dc1d2, 2024/05/09
